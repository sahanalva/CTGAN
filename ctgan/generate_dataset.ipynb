{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from importlib import reload\n",
    "import synthesizer,transformer,sampler,conditional,models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = \"/Users/sahanalva/Downloads/application_train.csv\" \n",
    "SAVE_PROCESSED_ORIGNAL_DATA_PATH = \"/Users/sahanalva/Counterfactual Research/home_credit_sample_processed_data.csv\"\n",
    "SAVE_MODEL_PATH = \"/Users/sahanalva/Counterfactual Research/home_credit_sample.pt\"\n",
    "BLACK_BOX_PATH = \"/Users/sahanalva/Counterfactual Research/tree_model_test.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_FILE_PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required_cols = ['loan_type_name','loan_purpose_name','agency_abbr','property_type_name','state_name','preapproval_name', 'purchaser_type_name','loan_amount_000s', 'tract_to_msamd_income', 'population','number_of_1_to_4_family_units','minority_population','applicant_income_000s','hud_median_family_income','number_of_owner_occupied_units']\n",
    "# conditional_cols = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_string(df, discrete_columns):\n",
    "    for col in discrete_columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_columns = ['NAME_CONTRACT_TYPE',\n",
    " 'CODE_GENDER',\n",
    " 'FLAG_OWN_CAR',\n",
    " 'FLAG_OWN_REALTY',\n",
    " 'NAME_TYPE_SUITE',\n",
    " 'NAME_INCOME_TYPE']\n",
    "#  'NAME_EDUCATION_TYPE',\n",
    "#  'NAME_FAMILY_STATUS',\n",
    "#  'NAME_HOUSING_TYPE',\n",
    "#  'OCCUPATION_TYPE',\n",
    "#  'WEEKDAY_APPR_PROCESS_START',\n",
    "#  'ORGANIZATION_TYPE', \n",
    "#  'CNT_CHILDREN',\n",
    "#  'CNT_FAM_MEMBERS',\n",
    "#  'REG_REGION_NOT_LIVE_REGION']\n",
    "\n",
    "numeric_columns= ['AMT_INCOME_TOTAL',\n",
    "        'AMT_CREDIT',\n",
    "        'AMT_ANNUITY',\n",
    "        'EXT_SOURCE_1',\n",
    "        'EXT_SOURCE_2',\n",
    "        'EXT_SOURCE_3']\n",
    "        #'DAYS_LAST_PHONE_CHANGE',\n",
    "        # 'AMT_GOODS_PRICE',\n",
    "        # 'REGION_POPULATION_RELATIVE',\n",
    "        # 'DAYS_BIRTH',\n",
    "        # 'DAYS_EMPLOYED',\n",
    "        # 'DAYS_REGISTRATION',\n",
    "        # 'DAYS_ID_PUBLISH']\n",
    "\n",
    "required_cols = discrete_columns + numeric_columns\n",
    "conditional_cols = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[required_cols]\n",
    "df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "df[discrete_columns] = df[discrete_columns].fillna(\"NA\")\n",
    "df = convert_to_string(df,discrete_columns)\n",
    "#df = _factorize_categoricals(df, discrete_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NAME_CONTRACT_TYPE            False\n",
       "CODE_GENDER                   False\n",
       "FLAG_OWN_CAR                  False\n",
       "FLAG_OWN_REALTY               False\n",
       "NAME_TYPE_SUITE               False\n",
       "NAME_INCOME_TYPE              False\n",
       "NAME_EDUCATION_TYPE           False\n",
       "NAME_FAMILY_STATUS            False\n",
       "NAME_HOUSING_TYPE             False\n",
       "OCCUPATION_TYPE               False\n",
       "WEEKDAY_APPR_PROCESS_START    False\n",
       "ORGANIZATION_TYPE             False\n",
       "CNT_CHILDREN                  False\n",
       "CNT_FAM_MEMBERS               False\n",
       "REG_REGION_NOT_LIVE_REGION    False\n",
       "AMT_INCOME_TOTAL              False\n",
       "AMT_CREDIT                    False\n",
       "AMT_ANNUITY                   False\n",
       "AMT_GOODS_PRICE               False\n",
       "REGION_POPULATION_RELATIVE    False\n",
       "DAYS_BIRTH                    False\n",
       "DAYS_EMPLOYED                 False\n",
       "DAYS_REGISTRATION             False\n",
       "DAYS_ID_PUBLISH               False\n",
       "EXT_SOURCE_1                  False\n",
       "EXT_SOURCE_2                  False\n",
       "EXT_SOURCE_3                  False\n",
       "DAYS_LAST_PHONE_CHANGE        False\n",
       "dtype: bool"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# df = df[required_cols]\n",
    "# nunique = df.apply(pd.Series.nunique)\n",
    "# discrete_columns = nunique[nunique < 100].index\n",
    "# numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "# object_columns = df.select_dtypes(include=['object']).columns\n",
    "# df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "# df[object_columns] = df[object_columns].fillna(\"NA\")\n",
    "\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.2572665214538574, Conditional Loss:0.8849444389343262, Flip Loss:1.1966543197631836\n",
      "Base Loss:1.1964364051818848, Conditional Loss:4.2554612159729, Flip Loss:1.18965744972229\n",
      "Base Loss:1.2571181058883667, Conditional Loss:3.814457893371582, Flip Loss:1.1864235401153564\n",
      "Base Loss:1.2745620012283325, Conditional Loss:3.43792462348938, Flip Loss:1.1749849319458008\n",
      "Base Loss:1.3188954591751099, Conditional Loss:2.8224329948425293, Flip Loss:1.1692992448806763\n",
      "Base Loss:1.1527653932571411, Conditional Loss:2.213632345199585, Flip Loss:1.1815696954727173\n",
      "Base Loss:1.2469969987869263, Conditional Loss:4.868907451629639, Flip Loss:1.1761478185653687\n",
      "Condtional Cross Entropy Loss tensor(4.8689, grad_fn=<DivBackward0>)\n",
      "Epoch 199, Loss G: 7.2921, Loss D: -0.1936, Loss Conditional D: -0.7309\n",
      "Base Loss:1.1784964799880981, Conditional Loss:0.8764562010765076, Flip Loss:1.1900064945220947\n",
      "Base Loss:1.1618523597717285, Conditional Loss:3.9168031215667725, Flip Loss:1.1881511211395264\n",
      "Base Loss:1.1477744579315186, Conditional Loss:3.447690725326538, Flip Loss:1.1730602979660034\n",
      "Base Loss:1.1640937328338623, Conditional Loss:3.795940399169922, Flip Loss:1.183868169784546\n",
      "Base Loss:1.2598127126693726, Conditional Loss:2.1717422008514404, Flip Loss:1.1773632764816284\n",
      "Base Loss:1.087072491645813, Conditional Loss:0.013870908878743649, Flip Loss:1.1770975589752197\n",
      "Base Loss:1.1563163995742798, Conditional Loss:0.013774759136140347, Flip Loss:1.1712590456008911\n",
      "Base Loss:1.2615333795547485, Conditional Loss:3.8326802253723145, Flip Loss:1.2077845335006714\n",
      "Base Loss:1.2624402046203613, Conditional Loss:0.04957906901836395, Flip Loss:1.17632257938385\n",
      "Base Loss:1.2141735553741455, Conditional Loss:0.013664178550243378, Flip Loss:1.1640937328338623\n",
      "Base Loss:1.2486813068389893, Conditional Loss:0.2726629972457886, Flip Loss:1.1766456365585327\n",
      "Base Loss:1.0683965682983398, Conditional Loss:0.7272194623947144, Flip Loss:1.1697319746017456\n",
      "Base Loss:1.2707184553146362, Conditional Loss:0.12685616314411163, Flip Loss:1.1804980039596558\n",
      "Base Loss:1.2035075426101685, Conditional Loss:2.2216103076934814, Flip Loss:1.1722086668014526\n",
      "Base Loss:1.1817433834075928, Conditional Loss:3.8194332122802734, Flip Loss:1.1721584796905518\n",
      "Base Loss:1.272851824760437, Conditional Loss:3.449082374572754, Flip Loss:1.197813630104065\n",
      "Base Loss:1.1937047243118286, Conditional Loss:2.5259504318237305, Flip Loss:1.1904395818710327\n",
      "Base Loss:1.1981273889541626, Conditional Loss:0.7428432106971741, Flip Loss:1.1927868127822876\n",
      "Base Loss:1.293856143951416, Conditional Loss:1.5328823328018188, Flip Loss:1.1951904296875\n",
      "Base Loss:1.1990907192230225, Conditional Loss:0.12598764896392822, Flip Loss:1.1851590871810913\n",
      "Base Loss:1.2668980360031128, Conditional Loss:4.47699499130249, Flip Loss:1.1830999851226807\n",
      "Base Loss:1.3134006261825562, Conditional Loss:3.8133347034454346, Flip Loss:1.1834739446640015\n",
      "Base Loss:1.1977627277374268, Conditional Loss:2.2586941719055176, Flip Loss:1.1886321306228638\n",
      "Base Loss:1.1357908248901367, Conditional Loss:0.5472422242164612, Flip Loss:1.19611656665802\n",
      "Base Loss:1.149117350578308, Conditional Loss:0.323393315076828, Flip Loss:1.1604608297348022\n",
      "Base Loss:1.4032973051071167, Conditional Loss:2.910581350326538, Flip Loss:1.187461018562317\n",
      "Base Loss:1.3250956535339355, Conditional Loss:3.377915859222412, Flip Loss:1.1895860433578491\n",
      "Base Loss:1.29897141456604, Conditional Loss:0.6733846664428711, Flip Loss:1.2192217111587524\n",
      "Base Loss:1.2966737747192383, Conditional Loss:1.404894232749939, Flip Loss:1.215685248374939\n",
      "Base Loss:1.3617241382598877, Conditional Loss:3.56046986579895, Flip Loss:1.212166428565979\n",
      "Base Loss:1.3006824254989624, Conditional Loss:0.18797551095485687, Flip Loss:1.190094232559204\n",
      "Base Loss:1.288478136062622, Conditional Loss:1.4069346189498901, Flip Loss:1.1630672216415405\n",
      "Base Loss:1.3292688131332397, Conditional Loss:3.548159122467041, Flip Loss:1.196018934249878\n",
      "Base Loss:1.3728750944137573, Conditional Loss:4.767971992492676, Flip Loss:1.1817371845245361\n",
      "Base Loss:1.373684048652649, Conditional Loss:5.115893840789795, Flip Loss:1.1880824565887451\n",
      "Base Loss:1.211910367012024, Conditional Loss:0.1721581220626831, Flip Loss:1.1871733665466309\n",
      "Base Loss:1.2892342805862427, Conditional Loss:4.848246097564697, Flip Loss:1.2159498929977417\n",
      "Base Loss:1.355971336364746, Conditional Loss:4.846843242645264, Flip Loss:1.1871150732040405\n",
      "Base Loss:1.2587767839431763, Conditional Loss:4.537741184234619, Flip Loss:1.2072279453277588\n",
      "Base Loss:1.2222609519958496, Conditional Loss:3.5238089561462402, Flip Loss:1.1739870309829712\n",
      "Base Loss:1.271023154258728, Conditional Loss:2.723891019821167, Flip Loss:1.1865166425704956\n",
      "Base Loss:1.1480231285095215, Conditional Loss:0.05005282908678055, Flip Loss:1.1893272399902344\n",
      "Base Loss:1.2785825729370117, Conditional Loss:2.212737798690796, Flip Loss:1.2167021036148071\n",
      "Base Loss:1.3521370887756348, Conditional Loss:2.2044739723205566, Flip Loss:1.1832212209701538\n",
      "Base Loss:1.1630940437316895, Conditional Loss:2.7196547985076904, Flip Loss:1.1832690238952637\n",
      "Base Loss:1.1232287883758545, Conditional Loss:3.080376148223877, Flip Loss:1.1762076616287231\n",
      "Base Loss:1.2182749509811401, Conditional Loss:4.207370281219482, Flip Loss:1.1729624271392822\n",
      "Base Loss:1.2857649326324463, Conditional Loss:1.4374926090240479, Flip Loss:1.1713355779647827\n",
      "Base Loss:1.2073314189910889, Conditional Loss:4.8178558349609375, Flip Loss:1.174665927886963\n",
      "Base Loss:1.13567316532135, Conditional Loss:1.7591145038604736, Flip Loss:1.190022349357605\n",
      "Base Loss:1.309807538986206, Conditional Loss:2.615830183029175, Flip Loss:1.174203872680664\n",
      "Base Loss:1.2833952903747559, Conditional Loss:2.684208631515503, Flip Loss:1.213095784187317\n",
      "Base Loss:1.220634937286377, Conditional Loss:4.3651018142700195, Flip Loss:1.1859180927276611\n",
      "Base Loss:1.0362292528152466, Conditional Loss:2.8707098960876465, Flip Loss:1.1925528049468994\n",
      "Base Loss:1.0577232837677002, Conditional Loss:0.19033324718475342, Flip Loss:1.2109888792037964\n",
      "Base Loss:1.1496328115463257, Conditional Loss:0.528988242149353, Flip Loss:1.1850457191467285\n",
      "Base Loss:1.0570675134658813, Conditional Loss:1.4368970394134521, Flip Loss:1.201195240020752\n",
      "Base Loss:1.1662304401397705, Conditional Loss:4.8776140213012695, Flip Loss:1.1904414892196655\n",
      "Base Loss:1.1611272096633911, Conditional Loss:1.4403811693191528, Flip Loss:1.1933914422988892\n",
      "Base Loss:1.0736273527145386, Conditional Loss:3.387380361557007, Flip Loss:1.1862365007400513\n",
      "Base Loss:1.0490100383758545, Conditional Loss:0.32750847935676575, Flip Loss:1.2055641412734985\n",
      "Base Loss:1.1448336839675903, Conditional Loss:0.12739704549312592, Flip Loss:1.1684614419937134\n",
      "Base Loss:1.114848017692566, Conditional Loss:1.8640376329421997, Flip Loss:1.2070339918136597\n",
      "Base Loss:1.152372121810913, Conditional Loss:1.6713846921920776, Flip Loss:1.1612141132354736\n",
      "Base Loss:1.1331710815429688, Conditional Loss:3.4496443271636963, Flip Loss:1.1929714679718018\n",
      "Base Loss:1.178810954093933, Conditional Loss:1.679473876953125, Flip Loss:1.1860164403915405\n",
      "Base Loss:1.0266482830047607, Conditional Loss:0.8978696465492249, Flip Loss:1.199460506439209\n",
      "Base Loss:1.1649600267410278, Conditional Loss:2.7022533416748047, Flip Loss:1.1801365613937378\n",
      "Base Loss:1.1979094743728638, Conditional Loss:2.783538579940796, Flip Loss:1.1696574687957764\n",
      "Base Loss:1.089921474456787, Conditional Loss:0.013674814254045486, Flip Loss:1.1652872562408447\n",
      "Base Loss:1.0603593587875366, Conditional Loss:2.7170732021331787, Flip Loss:1.1824309825897217\n",
      "Base Loss:1.1399779319763184, Conditional Loss:1.7568492889404297, Flip Loss:1.186388373374939\n",
      "Base Loss:1.164615511894226, Conditional Loss:2.645728826522827, Flip Loss:1.2220972776412964\n",
      "Base Loss:1.1602977514266968, Conditional Loss:2.167043447494507, Flip Loss:1.178483009338379\n",
      "Base Loss:1.051529884338379, Conditional Loss:0.4172000586986542, Flip Loss:1.183962345123291\n",
      "Base Loss:1.1220401525497437, Conditional Loss:2.239773988723755, Flip Loss:1.1791199445724487\n",
      "Base Loss:0.992046058177948, Conditional Loss:0.15912778675556183, Flip Loss:1.1640442609786987\n",
      "Base Loss:0.9770799875259399, Conditional Loss:0.5625714659690857, Flip Loss:1.19020676612854\n",
      "Base Loss:1.1213390827178955, Conditional Loss:4.665728569030762, Flip Loss:1.1669495105743408\n",
      "Base Loss:1.1153950691223145, Conditional Loss:2.586925745010376, Flip Loss:1.2176804542541504\n",
      "Base Loss:1.0840671062469482, Conditional Loss:0.05261930450797081, Flip Loss:1.1825437545776367\n",
      "Base Loss:1.1385470628738403, Conditional Loss:0.7056894898414612, Flip Loss:1.1726337671279907\n",
      "Base Loss:1.1354656219482422, Conditional Loss:3.7931320667266846, Flip Loss:1.1913840770721436\n",
      "Base Loss:1.3294317722320557, Conditional Loss:3.4335694313049316, Flip Loss:1.1812838315963745\n",
      "Base Loss:1.2058498859405518, Conditional Loss:4.408674716949463, Flip Loss:1.1878033876419067\n",
      "Base Loss:1.1025127172470093, Conditional Loss:0.12374382466077805, Flip Loss:1.1718814373016357\n",
      "Base Loss:1.1578888893127441, Conditional Loss:3.6966753005981445, Flip Loss:1.179108738899231\n",
      "Base Loss:1.1443603038787842, Conditional Loss:0.8619269728660583, Flip Loss:1.2040833234786987\n",
      "Base Loss:1.1164546012878418, Conditional Loss:4.864397048950195, Flip Loss:1.1693171262741089\n",
      "Base Loss:1.1005092859268188, Conditional Loss:2.201737403869629, Flip Loss:1.194934606552124\n",
      "Base Loss:1.0437345504760742, Conditional Loss:1.3829137086868286, Flip Loss:1.172084093093872\n",
      "Base Loss:1.1369256973266602, Conditional Loss:2.7679007053375244, Flip Loss:1.181792974472046\n",
      "Base Loss:1.102013111114502, Conditional Loss:0.1281510293483734, Flip Loss:1.1813374757766724\n",
      "Base Loss:1.1293823719024658, Conditional Loss:2.7108824253082275, Flip Loss:1.2047922611236572\n",
      "Base Loss:1.1701879501342773, Conditional Loss:0.28425368666648865, Flip Loss:1.1984705924987793\n",
      "Base Loss:1.2066842317581177, Conditional Loss:0.7171006798744202, Flip Loss:1.1831194162368774\n",
      "Base Loss:1.0706270933151245, Conditional Loss:0.5612531304359436, Flip Loss:1.1726362705230713\n",
      "Base Loss:1.1025158166885376, Conditional Loss:0.0476597398519516, Flip Loss:1.1728885173797607\n",
      "Base Loss:1.1476017236709595, Conditional Loss:0.17764605581760406, Flip Loss:1.1590992212295532\n",
      "Base Loss:1.200797438621521, Conditional Loss:3.756160020828247, Flip Loss:1.1868525743484497\n",
      "Base Loss:1.1117631196975708, Conditional Loss:3.8136188983917236, Flip Loss:1.1690360307693481\n",
      "Base Loss:1.2110915184020996, Conditional Loss:4.501462459564209, Flip Loss:1.2047922611236572\n",
      "Base Loss:1.1801724433898926, Conditional Loss:3.2595155239105225, Flip Loss:1.1755917072296143\n",
      "Base Loss:1.1349769830703735, Conditional Loss:1.433976173400879, Flip Loss:1.2303612232208252\n",
      "Base Loss:1.1421022415161133, Conditional Loss:2.2027764320373535, Flip Loss:1.1838823556900024\n",
      "Base Loss:1.25102961063385, Conditional Loss:4.862433433532715, Flip Loss:1.1904727220535278\n",
      "Base Loss:1.2232930660247803, Conditional Loss:4.568231105804443, Flip Loss:1.1794214248657227\n",
      "Base Loss:1.213830828666687, Conditional Loss:2.681910514831543, Flip Loss:1.1911426782608032\n",
      "Base Loss:1.236485242843628, Conditional Loss:4.511722087860107, Flip Loss:1.1661126613616943\n",
      "Base Loss:1.295279860496521, Conditional Loss:3.272620677947998, Flip Loss:1.1901386976242065\n",
      "Base Loss:1.1893455982208252, Conditional Loss:4.495849609375, Flip Loss:1.1764389276504517\n",
      "Base Loss:1.1826567649841309, Conditional Loss:1.7290284633636475, Flip Loss:1.1873619556427002\n",
      "Base Loss:1.2585673332214355, Conditional Loss:4.7165913581848145, Flip Loss:1.2025269269943237\n",
      "Base Loss:1.2017837762832642, Conditional Loss:2.1574313640594482, Flip Loss:1.205406665802002\n",
      "Base Loss:1.0797117948532104, Conditional Loss:3.8770182132720947, Flip Loss:1.1603344678878784\n",
      "Base Loss:1.1784189939498901, Conditional Loss:2.6977012157440186, Flip Loss:1.1710894107818604\n",
      "Base Loss:1.1782333850860596, Conditional Loss:1.8211296796798706, Flip Loss:1.1844569444656372\n",
      "Base Loss:1.1198726892471313, Conditional Loss:0.13425040245056152, Flip Loss:1.1694562435150146\n",
      "Base Loss:1.1999495029449463, Conditional Loss:1.850042462348938, Flip Loss:1.1776759624481201\n",
      "Base Loss:1.1804003715515137, Conditional Loss:0.5469186305999756, Flip Loss:1.1796414852142334\n",
      "Base Loss:1.2539695501327515, Conditional Loss:2.6578986644744873, Flip Loss:1.1770167350769043\n",
      "Base Loss:1.3038541078567505, Conditional Loss:3.4065651893615723, Flip Loss:1.1801729202270508\n",
      "Base Loss:1.248786449432373, Conditional Loss:5.032889366149902, Flip Loss:1.1716701984405518\n",
      "Base Loss:1.363566279411316, Conditional Loss:0.9066227674484253, Flip Loss:1.1788978576660156\n",
      "Base Loss:1.3281261920928955, Conditional Loss:3.883212089538574, Flip Loss:1.1620845794677734\n",
      "Base Loss:1.301464557647705, Conditional Loss:4.0951924324035645, Flip Loss:1.1906650066375732\n",
      "Base Loss:1.3183174133300781, Conditional Loss:0.2681029736995697, Flip Loss:1.1976994276046753\n",
      "Base Loss:1.3096383810043335, Conditional Loss:0.013673553243279457, Flip Loss:1.1728285551071167\n",
      "Base Loss:1.3379333019256592, Conditional Loss:0.7540671229362488, Flip Loss:1.1785637140274048\n",
      "Base Loss:1.3165315389633179, Conditional Loss:4.904105186462402, Flip Loss:1.18831467628479\n",
      "Base Loss:1.2117974758148193, Conditional Loss:2.7080183029174805, Flip Loss:1.17223060131073\n",
      "Base Loss:1.279883623123169, Conditional Loss:0.17373155057430267, Flip Loss:1.194800853729248\n",
      "Base Loss:1.3292065858840942, Conditional Loss:0.049931082874536514, Flip Loss:1.1535106897354126\n",
      "Base Loss:1.2225172519683838, Conditional Loss:3.268507242202759, Flip Loss:1.187632441520691\n",
      "Base Loss:1.3221126794815063, Conditional Loss:2.7022488117218018, Flip Loss:1.1825213432312012\n",
      "Base Loss:1.3203544616699219, Conditional Loss:0.5642234086990356, Flip Loss:1.19936203956604\n",
      "Base Loss:1.4181157350540161, Conditional Loss:0.013678113929927349, Flip Loss:1.208498239517212\n",
      "Base Loss:1.3927839994430542, Conditional Loss:0.1270541101694107, Flip Loss:1.1549248695373535\n",
      "Base Loss:1.3733494281768799, Conditional Loss:3.8142573833465576, Flip Loss:1.1940715312957764\n",
      "Base Loss:1.3461363315582275, Conditional Loss:0.325023353099823, Flip Loss:1.1988497972488403\n",
      "Base Loss:1.2779285907745361, Conditional Loss:0.13009749352931976, Flip Loss:1.1586413383483887\n",
      "Base Loss:1.1677706241607666, Conditional Loss:4.885986804962158, Flip Loss:1.1756988763809204\n",
      "Base Loss:1.3630071878433228, Conditional Loss:1.680558443069458, Flip Loss:1.1891686916351318\n",
      "Base Loss:1.3666234016418457, Conditional Loss:0.1315033882856369, Flip Loss:1.1793932914733887\n",
      "Base Loss:1.334579586982727, Conditional Loss:4.856952667236328, Flip Loss:1.1836668252944946\n",
      "Base Loss:1.2026236057281494, Conditional Loss:0.4153164327144623, Flip Loss:1.1955010890960693\n",
      "Base Loss:1.2515463829040527, Conditional Loss:3.381256341934204, Flip Loss:1.1673988103866577\n",
      "Base Loss:1.1947669982910156, Conditional Loss:0.13367992639541626, Flip Loss:1.1718239784240723\n",
      "Base Loss:1.269058346748352, Conditional Loss:3.3044848442077637, Flip Loss:1.1804478168487549\n",
      "Base Loss:1.2827820777893066, Conditional Loss:0.18212078511714935, Flip Loss:1.1719326972961426\n",
      "Base Loss:1.174823522567749, Conditional Loss:5.215919494628906, Flip Loss:1.178883671760559\n",
      "Base Loss:1.2373040914535522, Conditional Loss:3.505659341812134, Flip Loss:1.175378680229187\n",
      "Base Loss:1.3091187477111816, Conditional Loss:5.066763877868652, Flip Loss:1.1959173679351807\n",
      "Base Loss:1.1782742738723755, Conditional Loss:0.8928624391555786, Flip Loss:1.1860613822937012\n",
      "Base Loss:1.1085354089736938, Conditional Loss:4.785020351409912, Flip Loss:1.1932259798049927\n",
      "Base Loss:1.1810425519943237, Conditional Loss:2.8200387954711914, Flip Loss:1.1836119890213013\n",
      "Base Loss:1.2511813640594482, Conditional Loss:2.2350049018859863, Flip Loss:1.2246841192245483\n",
      "Base Loss:1.1673468351364136, Conditional Loss:0.01369947288185358, Flip Loss:1.1930503845214844\n",
      "Base Loss:1.1705901622772217, Conditional Loss:3.28303861618042, Flip Loss:1.1891767978668213\n",
      "Base Loss:1.1047481298446655, Conditional Loss:0.049205344170331955, Flip Loss:1.1825363636016846\n",
      "Base Loss:1.1086078882217407, Conditional Loss:1.528511881828308, Flip Loss:1.185666799545288\n",
      "Base Loss:1.01351797580719, Conditional Loss:0.5448563694953918, Flip Loss:1.1794377565383911\n",
      "Base Loss:1.0560731887817383, Conditional Loss:1.4414393901824951, Flip Loss:1.1894710063934326\n",
      "Base Loss:1.120355248451233, Conditional Loss:0.6871491074562073, Flip Loss:1.184932827949524\n",
      "Base Loss:1.172197699546814, Conditional Loss:2.553230047225952, Flip Loss:1.1891840696334839\n",
      "Base Loss:1.1262404918670654, Conditional Loss:0.5081560611724854, Flip Loss:1.2109599113464355\n",
      "Base Loss:1.2123570442199707, Conditional Loss:0.33516016602516174, Flip Loss:1.1788231134414673\n",
      "Base Loss:1.1527401208877563, Conditional Loss:1.5188981294631958, Flip Loss:1.1791895627975464\n",
      "Base Loss:1.2183620929718018, Conditional Loss:3.8466944694519043, Flip Loss:1.184248685836792\n",
      "Base Loss:1.1301493644714355, Conditional Loss:0.2923029959201813, Flip Loss:1.19472336769104\n",
      "Base Loss:1.1605066061019897, Conditional Loss:5.127450942993164, Flip Loss:1.2034578323364258\n",
      "Base Loss:1.1965450048446655, Conditional Loss:3.842804193496704, Flip Loss:1.1901415586471558\n",
      "Base Loss:1.2916151285171509, Conditional Loss:1.454470157623291, Flip Loss:1.193281650543213\n",
      "Base Loss:1.298683524131775, Conditional Loss:2.683892250061035, Flip Loss:1.2026606798171997\n",
      "Base Loss:1.2192754745483398, Conditional Loss:0.5677251219749451, Flip Loss:1.1987873315811157\n",
      "Base Loss:1.1461600065231323, Conditional Loss:0.013917368836700916, Flip Loss:1.1825286149978638\n",
      "Base Loss:1.2624099254608154, Conditional Loss:2.1684160232543945, Flip Loss:1.179904580116272\n",
      "Base Loss:1.339978814125061, Conditional Loss:2.6876535415649414, Flip Loss:1.1957687139511108\n",
      "Base Loss:1.107292890548706, Conditional Loss:0.834445595741272, Flip Loss:1.187097191810608\n",
      "Base Loss:1.220739483833313, Conditional Loss:4.766911506652832, Flip Loss:1.2035012245178223\n",
      "Base Loss:1.2352851629257202, Conditional Loss:0.32903850078582764, Flip Loss:1.2097480297088623\n",
      "Base Loss:1.105907917022705, Conditional Loss:0.8800917863845825, Flip Loss:1.2233010530471802\n",
      "Base Loss:1.2223843336105347, Conditional Loss:0.047009386122226715, Flip Loss:1.1880742311477661\n",
      "Base Loss:1.2157822847366333, Conditional Loss:0.6615397334098816, Flip Loss:1.1893140077590942\n",
      "Base Loss:1.244948148727417, Conditional Loss:3.385878562927246, Flip Loss:1.1512101888656616\n",
      "Base Loss:1.2289820909500122, Conditional Loss:0.16606557369232178, Flip Loss:1.1642484664916992\n",
      "Base Loss:1.2179310321807861, Conditional Loss:2.1010000705718994, Flip Loss:1.20357084274292\n",
      "Base Loss:1.0893876552581787, Conditional Loss:0.8985418081283569, Flip Loss:1.2073646783828735\n",
      "Base Loss:1.2979257106781006, Conditional Loss:4.388925552368164, Flip Loss:1.1859856843948364\n",
      "Base Loss:1.2006365060806274, Conditional Loss:2.8497047424316406, Flip Loss:1.1758685111999512\n",
      "Base Loss:1.3277099132537842, Conditional Loss:4.83037805557251, Flip Loss:1.1832759380340576\n",
      "Base Loss:1.1725784540176392, Conditional Loss:3.8569564819335938, Flip Loss:1.202130913734436\n",
      "Base Loss:1.1905970573425293, Conditional Loss:3.82789945602417, Flip Loss:1.187255620956421\n",
      "Base Loss:1.2806472778320312, Conditional Loss:2.7066099643707275, Flip Loss:1.1879206895828247\n",
      "Base Loss:1.2400778532028198, Conditional Loss:0.013833708129823208, Flip Loss:1.1953887939453125\n",
      "Base Loss:1.3703538179397583, Conditional Loss:3.827284574508667, Flip Loss:1.1882526874542236\n",
      "Base Loss:1.2146145105361938, Conditional Loss:0.5243616104125977, Flip Loss:1.1770797967910767\n",
      "Base Loss:1.2694002389907837, Conditional Loss:0.1704743206501007, Flip Loss:1.213852047920227\n",
      "Base Loss:1.3202797174453735, Conditional Loss:0.31376713514328003, Flip Loss:1.2039506435394287\n",
      "Base Loss:1.3298792839050293, Conditional Loss:4.518892765045166, Flip Loss:1.1901719570159912\n",
      "Condtional Cross Entropy Loss tensor(4.5189, grad_fn=<DivBackward0>)\n",
      "Epoch 200, Loss G: 7.0389, Loss D: -0.1470, Loss Conditional D: -0.6269\n"
     ]
    }
   ],
   "source": [
    "df = df[0:100000]\n",
    "model = synthesizer.CTGANSynthesizer(embedding_dim = 128, batch_size=500)\n",
    "model.fit(df, 1, BLACK_BOX_PATH, discrete_columns, conditional_cols, 200,log_frequency= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.3585, grad_fn=<DivBackward0>)\n",
      "tensor(2.4237, grad_fn=<DivBackward0>)\n",
      "tensor(2.6591, grad_fn=<DivBackward0>)\n",
      "tensor(2.5600, grad_fn=<DivBackward0>)\n",
      "tensor(2.5097, grad_fn=<DivBackward0>)\n",
      "tensor(2.4086, grad_fn=<DivBackward0>)\n",
      "tensor(2.3558, grad_fn=<DivBackward0>)\n",
      "tensor(2.4477, grad_fn=<DivBackward0>)\n",
      "tensor(2.4152, grad_fn=<DivBackward0>)\n",
      "tensor(2.4922, grad_fn=<DivBackward0>)\n",
      "tensor(2.3866, grad_fn=<DivBackward0>)\n",
      "tensor(2.3268, grad_fn=<DivBackward0>)\n",
      "tensor(2.3892, grad_fn=<DivBackward0>)\n",
      "tensor(2.4448, grad_fn=<DivBackward0>)\n",
      "tensor(2.4101, grad_fn=<DivBackward0>)\n",
      "tensor(2.3630, grad_fn=<DivBackward0>)\n",
      "tensor(2.5483, grad_fn=<DivBackward0>)\n",
      "tensor(2.4737, grad_fn=<DivBackward0>)\n",
      "tensor(2.4480, grad_fn=<DivBackward0>)\n",
      "tensor(2.2438, grad_fn=<DivBackward0>)\n",
      "tensor(2.3413, grad_fn=<DivBackward0>)\n",
      "tensor(2.3735, grad_fn=<DivBackward0>)\n",
      "tensor(2.5830, grad_fn=<DivBackward0>)\n",
      "tensor(2.4175, grad_fn=<DivBackward0>)\n",
      "tensor(2.4826, grad_fn=<DivBackward0>)\n",
      "tensor(2.4983, grad_fn=<DivBackward0>)\n",
      "tensor(2.3864, grad_fn=<DivBackward0>)\n",
      "tensor(2.4270, grad_fn=<DivBackward0>)\n",
      "tensor(2.5205, grad_fn=<DivBackward0>)\n",
      "tensor(2.4275, grad_fn=<DivBackward0>)\n",
      "tensor(2.5415, grad_fn=<DivBackward0>)\n",
      "tensor(2.3687, grad_fn=<DivBackward0>)\n",
      "tensor(2.4560, grad_fn=<DivBackward0>)\n",
      "tensor(2.4108, grad_fn=<DivBackward0>)\n",
      "tensor(2.3925, grad_fn=<DivBackward0>)\n",
      "tensor(2.2156, grad_fn=<DivBackward0>)\n",
      "tensor(2.4324, grad_fn=<DivBackward0>)\n",
      "tensor(2.4796, grad_fn=<DivBackward0>)\n",
      "tensor(2.3532, grad_fn=<DivBackward0>)\n",
      "tensor(2.3213, grad_fn=<DivBackward0>)\n",
      "tensor(2.3560, grad_fn=<DivBackward0>)\n",
      "tensor(2.4308, grad_fn=<DivBackward0>)\n",
      "tensor(2.5871, grad_fn=<DivBackward0>)\n",
      "tensor(2.3565, grad_fn=<DivBackward0>)\n",
      "tensor(2.4268, grad_fn=<DivBackward0>)\n",
      "tensor(2.4850, grad_fn=<DivBackward0>)\n",
      "tensor(2.4321, grad_fn=<DivBackward0>)\n",
      "tensor(2.4773, grad_fn=<DivBackward0>)\n",
      "tensor(2.3102, grad_fn=<DivBackward0>)\n",
      "tensor(2.5521, grad_fn=<DivBackward0>)\n",
      "tensor(2.4322, grad_fn=<DivBackward0>)\n",
      "tensor(2.3437, grad_fn=<DivBackward0>)\n",
      "tensor(2.4862, grad_fn=<DivBackward0>)\n",
      "tensor(2.4416, grad_fn=<DivBackward0>)\n",
      "tensor(2.5766, grad_fn=<DivBackward0>)\n",
      "tensor(2.2836, grad_fn=<DivBackward0>)\n",
      "tensor(2.3311, grad_fn=<DivBackward0>)\n",
      "tensor(2.4738, grad_fn=<DivBackward0>)\n",
      "tensor(2.4202, grad_fn=<DivBackward0>)\n",
      "tensor(2.3566, grad_fn=<DivBackward0>)\n",
      "tensor(2.5601, grad_fn=<DivBackward0>)\n",
      "tensor(2.4711, grad_fn=<DivBackward0>)\n",
      "tensor(2.4633, grad_fn=<DivBackward0>)\n",
      "tensor(2.5701, grad_fn=<DivBackward0>)\n",
      "tensor(2.4304, grad_fn=<DivBackward0>)\n",
      "tensor(2.4219, grad_fn=<DivBackward0>)\n",
      "tensor(2.3843, grad_fn=<DivBackward0>)\n",
      "tensor(2.3806, grad_fn=<DivBackward0>)\n",
      "tensor(2.3933, grad_fn=<DivBackward0>)\n",
      "tensor(2.4055, grad_fn=<DivBackward0>)\n",
      "tensor(2.5884, grad_fn=<DivBackward0>)\n",
      "tensor(2.4985, grad_fn=<DivBackward0>)\n",
      "tensor(2.3456, grad_fn=<DivBackward0>)\n",
      "tensor(2.4542, grad_fn=<DivBackward0>)\n",
      "tensor(2.3839, grad_fn=<DivBackward0>)\n",
      "tensor(2.4478, grad_fn=<DivBackward0>)\n",
      "tensor(2.4667, grad_fn=<DivBackward0>)\n",
      "tensor(2.3063, grad_fn=<DivBackward0>)\n",
      "tensor(2.5997, grad_fn=<DivBackward0>)\n",
      "tensor(2.3935, grad_fn=<DivBackward0>)\n",
      "tensor(2.4052, grad_fn=<DivBackward0>)\n",
      "tensor(2.4902, grad_fn=<DivBackward0>)\n",
      "tensor(2.5574, grad_fn=<DivBackward0>)\n",
      "tensor(2.4206, grad_fn=<DivBackward0>)\n",
      "tensor(2.5442, grad_fn=<DivBackward0>)\n",
      "tensor(2.5446, grad_fn=<DivBackward0>)\n",
      "tensor(2.4768, grad_fn=<DivBackward0>)\n",
      "tensor(2.5250, grad_fn=<DivBackward0>)\n",
      "tensor(2.4520, grad_fn=<DivBackward0>)\n",
      "tensor(2.4771, grad_fn=<DivBackward0>)\n",
      "tensor(2.3932, grad_fn=<DivBackward0>)\n",
      "tensor(2.5003, grad_fn=<DivBackward0>)\n",
      "tensor(2.4205, grad_fn=<DivBackward0>)\n",
      "tensor(2.4757, grad_fn=<DivBackward0>)\n",
      "tensor(2.5217, grad_fn=<DivBackward0>)\n",
      "tensor(2.5090, grad_fn=<DivBackward0>)\n",
      "tensor(2.5646, grad_fn=<DivBackward0>)\n",
      "tensor(2.4448, grad_fn=<DivBackward0>)\n",
      "tensor(2.4498, grad_fn=<DivBackward0>)\n",
      "tensor(2.4524, grad_fn=<DivBackward0>)\n",
      "tensor(2.4827, grad_fn=<DivBackward0>)\n",
      "tensor(2.4843, grad_fn=<DivBackward0>)\n",
      "tensor(2.5466, grad_fn=<DivBackward0>)\n",
      "tensor(2.4428, grad_fn=<DivBackward0>)\n",
      "tensor(2.4581, grad_fn=<DivBackward0>)\n",
      "tensor(2.4630, grad_fn=<DivBackward0>)\n",
      "tensor(2.4294, grad_fn=<DivBackward0>)\n",
      "tensor(2.4483, grad_fn=<DivBackward0>)\n",
      "tensor(2.4180, grad_fn=<DivBackward0>)\n",
      "tensor(2.4788, grad_fn=<DivBackward0>)\n",
      "tensor(2.4214, grad_fn=<DivBackward0>)\n",
      "tensor(2.5588, grad_fn=<DivBackward0>)\n",
      "tensor(2.3665, grad_fn=<DivBackward0>)\n",
      "tensor(2.3775, grad_fn=<DivBackward0>)\n",
      "tensor(2.3885, grad_fn=<DivBackward0>)\n",
      "tensor(2.3537, grad_fn=<DivBackward0>)\n",
      "tensor(2.4365, grad_fn=<DivBackward0>)\n",
      "tensor(2.4066, grad_fn=<DivBackward0>)\n",
      "tensor(2.5163, grad_fn=<DivBackward0>)\n",
      "tensor(2.3329, grad_fn=<DivBackward0>)\n",
      "tensor(2.3743, grad_fn=<DivBackward0>)\n",
      "tensor(2.5151, grad_fn=<DivBackward0>)\n",
      "tensor(2.5087, grad_fn=<DivBackward0>)\n",
      "tensor(2.3135, grad_fn=<DivBackward0>)\n",
      "tensor(2.3841, grad_fn=<DivBackward0>)\n",
      "tensor(2.5055, grad_fn=<DivBackward0>)\n",
      "tensor(2.4960, grad_fn=<DivBackward0>)\n",
      "tensor(2.5235, grad_fn=<DivBackward0>)\n",
      "tensor(2.3734, grad_fn=<DivBackward0>)\n",
      "tensor(2.3679, grad_fn=<DivBackward0>)\n",
      "tensor(2.4211, grad_fn=<DivBackward0>)\n",
      "tensor(2.4197, grad_fn=<DivBackward0>)\n",
      "tensor(2.5491, grad_fn=<DivBackward0>)\n",
      "tensor(2.4776, grad_fn=<DivBackward0>)\n",
      "tensor(2.5967, grad_fn=<DivBackward0>)\n",
      "tensor(2.4466, grad_fn=<DivBackward0>)\n",
      "tensor(2.4075, grad_fn=<DivBackward0>)\n",
      "tensor(2.4708, grad_fn=<DivBackward0>)\n",
      "tensor(2.4243, grad_fn=<DivBackward0>)\n",
      "tensor(2.2900, grad_fn=<DivBackward0>)\n",
      "tensor(2.4467, grad_fn=<DivBackward0>)\n",
      "tensor(2.4592, grad_fn=<DivBackward0>)\n",
      "tensor(2.3277, grad_fn=<DivBackward0>)\n",
      "tensor(2.4481, grad_fn=<DivBackward0>)\n",
      "tensor(2.4535, grad_fn=<DivBackward0>)\n",
      "tensor(2.4899, grad_fn=<DivBackward0>)\n",
      "tensor(2.4989, grad_fn=<DivBackward0>)\n",
      "tensor(2.5080, grad_fn=<DivBackward0>)\n",
      "tensor(2.4428, grad_fn=<DivBackward0>)\n",
      "tensor(2.3694, grad_fn=<DivBackward0>)\n",
      "tensor(2.3719, grad_fn=<DivBackward0>)\n",
      "tensor(2.4562, grad_fn=<DivBackward0>)\n",
      "tensor(2.3923, grad_fn=<DivBackward0>)\n",
      "tensor(2.4373, grad_fn=<DivBackward0>)\n",
      "tensor(2.2631, grad_fn=<DivBackward0>)\n",
      "tensor(2.4770, grad_fn=<DivBackward0>)\n",
      "tensor(2.5241, grad_fn=<DivBackward0>)\n",
      "tensor(2.6326, grad_fn=<DivBackward0>)\n",
      "tensor(2.4753, grad_fn=<DivBackward0>)\n",
      "tensor(2.4175, grad_fn=<DivBackward0>)\n",
      "tensor(2.5413, grad_fn=<DivBackward0>)\n",
      "tensor(2.4701, grad_fn=<DivBackward0>)\n",
      "tensor(2.3428, grad_fn=<DivBackward0>)\n",
      "tensor(2.4349, grad_fn=<DivBackward0>)\n",
      "tensor(2.3715, grad_fn=<DivBackward0>)\n",
      "tensor(2.4998, grad_fn=<DivBackward0>)\n",
      "tensor(2.5104, grad_fn=<DivBackward0>)\n",
      "tensor(2.3185, grad_fn=<DivBackward0>)\n",
      "tensor(2.3102, grad_fn=<DivBackward0>)\n",
      "tensor(2.4492, grad_fn=<DivBackward0>)\n",
      "tensor(2.2544, grad_fn=<DivBackward0>)\n",
      "tensor(2.3864, grad_fn=<DivBackward0>)\n",
      "tensor(2.3955, grad_fn=<DivBackward0>)\n",
      "tensor(2.4435, grad_fn=<DivBackward0>)\n",
      "tensor(2.5012, grad_fn=<DivBackward0>)\n",
      "tensor(2.4259, grad_fn=<DivBackward0>)\n",
      "tensor(2.3290, grad_fn=<DivBackward0>)\n",
      "tensor(2.3713, grad_fn=<DivBackward0>)\n",
      "tensor(2.3519, grad_fn=<DivBackward0>)\n",
      "tensor(2.5080, grad_fn=<DivBackward0>)\n",
      "tensor(2.5514, grad_fn=<DivBackward0>)\n",
      "tensor(2.4417, grad_fn=<DivBackward0>)\n",
      "tensor(2.4222, grad_fn=<DivBackward0>)\n",
      "tensor(2.2582, grad_fn=<DivBackward0>)\n",
      "tensor(2.4174, grad_fn=<DivBackward0>)\n",
      "tensor(2.3542, grad_fn=<DivBackward0>)\n",
      "tensor(2.2318, grad_fn=<DivBackward0>)\n",
      "tensor(2.5610, grad_fn=<DivBackward0>)\n",
      "tensor(2.5880, grad_fn=<DivBackward0>)\n",
      "tensor(2.5160, grad_fn=<DivBackward0>)\n",
      "tensor(2.2188, grad_fn=<DivBackward0>)\n",
      "tensor(2.3399, grad_fn=<DivBackward0>)\n",
      "tensor(2.4044, grad_fn=<DivBackward0>)\n",
      "tensor(2.7479, grad_fn=<DivBackward0>)\n",
      "tensor(2.4906, grad_fn=<DivBackward0>)\n",
      "tensor(2.5697, grad_fn=<DivBackward0>)\n",
      "tensor(2.4143, grad_fn=<DivBackward0>)\n",
      "tensor(2.4568, grad_fn=<DivBackward0>)\n",
      "tensor(2.3661, grad_fn=<DivBackward0>)\n",
      "tensor(2.5393, grad_fn=<DivBackward0>)\n",
      "tensor(2.4704, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sampled = model.sample(100000,col_index= [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, SAVE_MODEL_PATH)\n",
    "df.to_csv(SAVE_MODEL_PATH, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = pickle.load(open(\"/Users/sahanalva/Counterfactual Research/tree_model_test.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_processed = _factorize_categoricals(sampled, discrete_columns)\n",
    "sampled_processed = xgb.DMatrix(data = sampled_processed)\n",
    "black_box_pred_prob = bst.predict(sampled_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _factorize_categoricals(df, discrete_columns):\n",
    "    for col in discrete_columns:\n",
    "        df[col], _ = pd.factorize(df[col])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "98946"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "sum(black_box_pred_prob < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "M      55951\n",
       "F      41929\n",
       "XNA     2120\n",
       "Name: CODE_GENDER, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "sampled['CODE_GENDER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('cfr': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7167c3f0fd91c62838d2a03be19a9914b3e996c69823a9473f944e0b4c425e9e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}